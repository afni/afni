diff -rupN orig/kernel.h svml/kernel.h
--- orig/kernel.h	2011-08-26 16:51:19.000000000 -0400
+++ svml/kernel.h	2011-08-26 17:22:03.000000000 -0400
@@ -12,18 +12,64 @@
 /* KERNEL_PARM is defined in svm_common.h The field 'custom' is reserved for */
 /* parameters of the user defined kernel. You can also access and use */
 /* the parameters of the other kernels. Just replace the line 
-             return((double)(1.0)); 
-   with your own kernel. */
+ * return((double)(1.0)); with your own kernel. */
 
-  /* Example: The following computes the polynomial kernel. sprod_ss
-              computes the inner product between two sparse vectors. 
+/* Example: The following computes the polynomial kernel. sprod_ss
+ * computes the inner product between two sparse vectors.
+ *
+ * return((CFLOAT)pow(kernel_parm->coef_lin*sprod_ss(a->words,b->words)
+ * +kernel_parm->coef_const,(double)kernel_parm->poly_degree)); */
 
-      return((CFLOAT)pow(kernel_parm->coef_lin*sprod_ss(a->words,b->words)
-             +kernel_parm->coef_const,(double)kernel_parm->poly_degree)); 
-  */
-
-double custom_kernel(KERNEL_PARM *kernel_parm, DOC *a, DOC *b) 
-     /* plug in you favorite kernel */                          
+double custom_kernel(KERNEL_PARM *kernel_parm, DOC *a, DOC *b)                         
 {
-  return((double)(1.0)); 
+  if (!strcmp(kernel_parm->custom,"complex1"))
+  {
+    /* JL Dec. 2008:
+     * Kernel to calculate inner product of two complex sparse vectors. */
+    
+    /* The real part has to be in the first half and the imaginary part in the
+     * second half of the WORD vector within the DOCs. This kernel is basically 
+     * the complex equivalent of sprod_ss.*/
+    
+    FNUM size_i, size_j, i, j;
+    FVAL Re_a, Re_b, Im_a, Im_b, Re_sum, Im_sum;
+    size_i=size_j=i=j=0;
+    Re_a=Re_b=Im_a=Im_b=Re_sum=Im_sum=0.0;
+    
+    while ((a->words[i]).wnum) {i++;}
+    while ((b->words[j]).wnum) {j++;}
+    size_i=i/2;
+    size_j=j/2;
+    
+    if (i%2 != 0 || j%2 != 0) {
+      fprintf(stderr,
+          "** ERROR (kernel.h): something is wrong with the complex-valued data\n"
+          "                     representation in the WORD structure.\n\n");
+      exit(0);
+    }
+
+    i=j=0;  
+    while (i < size_i && j < size_j) {
+      if ((a->words[i]).wnum > (b->words[j]).wnum) {j++;}
+      else if ((a->words[i]).wnum < (b->words[j]).wnum) {i++;}
+      else {
+        Re_a=a->words[i].weight;
+        Im_a=a->words[i+size_i].weight;
+        Re_b=b->words[j].weight;
+        Im_b=b->words[j+size_j].weight;
+          
+        Im_b=-1.0*Im_b;
+        Re_sum += Re_a*Re_b-Im_a*Im_b;
+        Im_sum += Re_a*Im_b+Im_a*Re_b;
+        
+        i++;
+        j++;
+      }
+    }
+    return((double)Re_sum);
+  }
+  else {
+    fprintf(stderr, "** ERROR (kernel.h): Custom kernel '%s' not implemented! "
+        "How did you get here? \n", kernel_parm->custom); exit(0);
+  }
 }
diff -rupN orig/svm_classify.c svml/svm_classify.c
--- orig/svm_classify.c	2011-08-26 16:51:19.000000000 -0400
+++ svml/svm_classify.c	2011-08-26 17:22:03.000000000 -0400
@@ -29,17 +29,18 @@ void print_help(void);
 
 int main (int argc, char* argv[])
 {
-  DOC doc;   /* test example */
-  long max_docs,max_words_doc,lld,llsv;
-  long max_sv,max_words_sv,totdoc=0;
+  DOC *doc;   /* test example */
+  WORD *words;
+  long max_docs,max_words_doc,lld;
+  long totdoc=0,queryid,slackid;
   long correct=0,incorrect=0,no_accuracy=0;
   long res_a=0,res_b=0,res_c=0,res_d=0,wnum,pred_format;
-  long i,j;
+  long j;
   double t1,runtime=0;
-  double dist,doc_label;
-  char *line; 
+  double dist,doc_label,costfactor;
+  char *line,*comment; 
   FILE *predfl,*docfl;
-  MODEL model; 
+  MODEL *model; 
 
   read_input_parameters(argc,argv,docfile,modelfile,predictionsfile,
 			&verbosity,&pred_format);
@@ -47,21 +48,15 @@ int main (int argc, char* argv[])
   nol_ll(docfile,&max_docs,&max_words_doc,&lld); /* scan size of input file */
   max_words_doc+=2;
   lld+=2;
-  nol_ll(modelfile,&max_sv,&max_words_sv,&llsv); /* scan size of model file */
-  max_words_sv+=2;
-  llsv+=2;
-
-  model.supvec = (DOC **)my_malloc(sizeof(DOC *)*max_sv);
-  model.alpha = (double *)my_malloc(sizeof(double)*max_sv);
 
   line = (char *)my_malloc(sizeof(char)*lld);
-  doc.words = (WORD *)my_malloc(sizeof(WORD)*(max_words_doc+10));
+  words = (WORD *)my_malloc(sizeof(WORD)*(max_words_doc+10));
 
-  read_model(modelfile,&model,max_words_sv,llsv);
+  model=read_model(modelfile);
 
-  if(model.kernel_parm.kernel_type == 0) { /* linear kernel */
+  if(model->kernel_parm.kernel_type == 0) { /* linear kernel */
     /* compute weight vector */
-    add_weight_vector_to_linear_model(&model);
+    add_weight_vector_to_linear_model(model);
   }
   
   if(verbosity>=2) {
@@ -75,21 +70,26 @@ int main (int argc, char* argv[])
 
   while((!feof(docfl)) && fgets(line,(int)lld,docfl)) {
     if(line[0] == '#') continue;  /* line contains comments */
-    parse_document(line,&doc,&doc_label,&wnum,max_words_doc);
+    parse_document(line,words,&doc_label,&queryid,&slackid,&costfactor,&wnum,
+		   max_words_doc,&comment);
     totdoc++;
-    if(model.kernel_parm.kernel_type == 0) {   /* linear kernel */
-      for(j=0;(doc.words[j]).wnum != 0;j++) {  /* Check if feature numbers   */
-	if((doc.words[j]).wnum>model.totwords) /* are not larger than in     */
-	  (doc.words[j]).wnum=0;               /* model. Remove feature if   */
+    if(model->kernel_parm.kernel_type == 0) {   /* linear kernel */
+      for(j=0;(words[j]).wnum != 0;j++) {  /* Check if feature numbers   */
+	if((words[j]).wnum>model->totwords) /* are not larger than in     */
+	  (words[j]).wnum=0;               /* model. Remove feature if   */
       }                                        /* necessary.                 */
+      doc = create_example(-1,0,0,0.0,create_svector(words,comment,1.0));
       t1=get_runtime();
-      dist=classify_example_linear(&model,&doc);
+      dist=classify_example_linear(model,doc);
       runtime+=(get_runtime()-t1);
+      free_example(doc,1);
     }
     else {                             /* non-linear kernel */
+      doc = create_example(-1,0,0,0.0,create_svector(words,comment,1.0));
       t1=get_runtime();
-      dist=classify_example(&model,&doc);
+      dist=classify_example(model,doc);
       runtime+=(get_runtime()-t1);
+      free_example(doc,1);
     }
     if(dist>0) {
       if(pred_format==0) { /* old weired output format */
@@ -117,16 +117,8 @@ int main (int argc, char* argv[])
     }
   }  
   free(line);
-  free(doc.words);
-  for(i=1;i<model.sv_num;i++) {
-    free(model.supvec[i]->words);
-    free(model.supvec[i]);
-  }
-  free(model.supvec);
-  free(model.alpha);
-  if(model.kernel_parm.kernel_type == 0) { /* linear kernel */
-    free(model.lin_weights);
-  }
+  free(words);
+  free_model(model,1);
 
   if(verbosity>=2) {
     printf("done\n");
@@ -189,7 +181,7 @@ void read_input_parameters(int argc, cha
 
 void print_help(void)
 {
-  printf("\nSVM-light %s: Support Vector Machine, classification module     %s\n",VERSION,VERSION_DATE);
+  printf("\nSVM-light %s: Support Vector Machine, classification module     %s\n",VERSION_SVMLIGHT, VERSION_DATE_SVMLIGHT);
   copyright_notice();
   printf("   usage: svm_classify [options] example_file model_file output_file\n\n");
   printf("options: -h         -> this help\n");
diff -rupN orig/svm_common.c svml/svm_common.c
--- orig/svm_common.c	2011-08-26 16:51:19.000000000 -0400
+++ svml/svm_common.c	2011-08-26 17:22:03.000000000 -0400
@@ -20,7 +20,6 @@
 # include "svm_common.h"
 # include "kernel.h"           /* this contains a user supplied kernel */
 
-long   verbosity;              /* verbosity level (0-4) */
 long   kernel_cache_statistic;
 
 double classify_example(MODEL *model, DOC *ex) 
@@ -71,21 +70,23 @@ double sprod_ss(WORD *a, WORD *b) 
 {
     register FVAL sum=0;
     register WORD *ai,*bj;
+    
     ai=a;
     bj=b;
     while (ai->wnum && bj->wnum) {
       if(ai->wnum > bj->wnum) {
-	bj++;
+        bj++;
       }
       else if (ai->wnum < bj->wnum) {
-	ai++;
+        ai++;
       }
       else {
-	sum+=ai->weight * bj->weight;
-	ai++;
-	bj++;
+        sum+=ai->weight * bj->weight;
+        ai++;
+        bj++;
       }
     }
+    
     return((double)sum);
 }
 
@@ -196,7 +197,7 @@ void add_vector_ns(double *vec_n, WORD *
     ai++;
   }
 }
-
+  
 double sprod_ns(double *vec_n, WORD *vec_s)
 {
   register double sum=0;
@@ -244,7 +245,7 @@ void read_model(char *modelfile, MODEL *
   { perror (modelfile); exit (1); }
 
   fscanf(modelfl,"SVM-light Version %s\n",version_buffer);
-  if(strcmp(version_buffer,VERSION)) {
+  if(strcmp(version_buffer,VERSION_SVMLIGHT)) {
     perror ("Version of model-file does not match version of svm_classify!"); 
     exit (1); 
   }
diff -rupN orig/svm_common.h svml/svm_common.h
--- orig/svm_common.h	2011-08-26 16:51:19.000000000 -0400
+++ svml/svm_common.h	2011-08-26 17:29:31.000000000 -0400
@@ -28,8 +28,12 @@
 # include <time.h> 
 # include <float.h>
 
-# define VERSION       "V5.00"
-# define VERSION_DATE  "30.06.02"
+/* JL: changed VERSION      to VERSION_SVMLIGHT      and
+ *             VERSION_DATE to VERSIOM_DATE_SVMLING
+ * to avoid conflicts with afni
+ */
+# define VERSION_SVMLIGHT       "V5.00"
+# define VERSION_DATE_SVMLIGHT  "30.06.02"
 
 # define CFLOAT  float       /* the type of float to use for caching */
                              /* kernel evaluations. Using float saves */
@@ -121,6 +125,10 @@ typedef struct learn_parm {
   double svm_unlabbound;
   double *svm_cost;            /* individual upper bounds for each var */
   long   totwords;             /* number of features */
+  
+  /* JL July 2011: Added maximum number of iterations */
+  long max_iterations;
+
 } LEARN_PARM;
 
 typedef struct kernel_parm {
@@ -220,7 +228,7 @@ void   copyright_notice(void);
    int isnan(double);
 # endif
 
-extern long   verbosity;              /* verbosity level (0-4) */
+static long   verbosity;              /* verbosity level (0-4) */
 extern long   kernel_cache_statistic;
 
 
diff -rupN orig/svm_hideo.c svml/svm_hideo.c
--- orig/svm_hideo.c	2011-08-26 16:51:19.000000000 -0400
+++ svml/svm_hideo.c	2011-08-26 17:22:03.000000000 -0400
@@ -21,7 +21,6 @@
 
 /* Common Block Declarations */
 
-long verbosity;
 
 # define PRIMAL_OPTIMAL      1
 # define DUAL_OPTIMAL        2
diff -rupN orig/svm_learn.c svml/svm_learn.c
--- orig/svm_learn.c	2011-08-26 16:51:19.000000000 -0400
+++ svml/svm_learn.c	2011-08-26 17:22:03.000000000 -0400
@@ -20,6 +20,7 @@
 # include "svm_common.h"
 # include "svm_learn.h"
 
+static long verbosity=1;
 
 /* interface to QP-solver */
 double *optimize_qp(QP *, double *, long, double *, LEARN_PARM *);
@@ -121,7 +122,7 @@ void svm_learn_classification(DOC *docs,
   if(learn_parm->svm_c == 0.0) {  /* default value for C */
     learn_parm->svm_c=1.0/(r_delta_avg*r_delta_avg);
     if(verbosity>=1) 
-      printf("Setting default regularization parameter C=%.4f\n",
+      printf(" + Setting default regularization parameter C=%.4f\n",
 	     learn_parm->svm_c);
   }
 
@@ -156,7 +157,7 @@ void svm_learn_classification(DOC *docs,
     }
   }
   if(verbosity>=2) {
-    printf("%ld positive, %ld negative, and %ld unlabeled examples.\n",trainpos,trainneg,totdoc-trainpos-trainneg); fflush(stdout);
+    printf(" + %ld positive, %ld negative, and %ld unlabeled examples.\n",trainpos,trainneg,totdoc-trainpos-trainneg); fflush(stdout);
   }
 
   /* caching makes no sense for linear kernel */
@@ -167,28 +168,28 @@ void svm_learn_classification(DOC *docs,
   if(transduction) {
     learn_parm->svm_iter_to_shrink=99999999;
     if(verbosity >= 1)
-      printf("\nDeactivating Shrinking due to an incompatibility with the transductive \nlearner in the current version.\n\n");
+      printf("\n++ Deactivating Shrinking due to an incompatibility with the transductive \nlearner in the current version.\n\n");
   }
 
   if(transduction && learn_parm->compute_loo) {
     learn_parm->compute_loo=0;
     if(verbosity >= 1)
-      printf("\nCannot compute leave-one-out estimates for transductive learner.\n\n");
+      printf("\n++ Cannot compute leave-one-out estimates for transductive learner.\n\n");
   }    
 
   if(learn_parm->remove_inconsistent && learn_parm->compute_loo) {
     learn_parm->compute_loo=0;
-    printf("\nCannot compute leave-one-out estimates when removing inconsistent examples.\n\n");
+    printf("\n++ Cannot compute leave-one-out estimates when removing inconsistent examples.\n\n");
   }    
 
   if(learn_parm->compute_loo && ((trainpos == 1) || (trainneg == 1))) {
     learn_parm->compute_loo=0;
-    printf("\nCannot compute leave-one-out with only one example in one class.\n\n");
+    printf("\n++ Cannot compute leave-one-out with only one example in one class.\n\n");
   }    
 
 
   if(verbosity==1) {
-    printf("Optimizing"); fflush(stdout);
+    printf(" + Optimizing"); fflush(stdout);
   }
 
   /* train the svm */
@@ -200,7 +201,7 @@ void svm_learn_classification(DOC *docs,
 				     (long)1);
   
   if(verbosity>=1) {
-    if(verbosity==1) printf("done. (%ld iterations)\n",iterations);
+    if(verbosity==1) printf("done . (%ld iterations)\n",iterations);
 
     misclassified=0;
     for(i=0;(i<totdoc);i++) { /* get final statistic */
@@ -208,12 +209,12 @@ void svm_learn_classification(DOC *docs,
 	misclassified++;
     }
 
-    printf("Optimization finished (%ld misclassified, maxdiff=%.5f).\n",
+    printf(" + Optimization finished (%ld misclassified, maxdiff=%.5f).\n",
 	   misclassified,maxdiff); 
 
     runtime_end=get_runtime();
     if(verbosity>=2) {
-      printf("Runtime in cpu-seconds: %.2f (%.2f%% for kernel/%.2f%% for optimizer/%.2f%% for final/%.2f%% for update/%.2f%% for model/%.2f%% for check/%.2f%% for select)\n",
+      printf(" + Runtime in cpu-seconds: %.2f (%.2f%% for kernel/%.2f%% for optimizer/%.2f%% for final/%.2f%% for update/%.2f%% for model/%.2f%% for check/%.2f%% for select)\n",
         ((float)runtime_end-(float)runtime_start)/100.0,
         (100.0*timing_profile.time_kernel)/(float)(runtime_end-runtime_start),
 	(100.0*timing_profile.time_opti)/(float)(runtime_end-runtime_start),
@@ -224,7 +225,7 @@ void svm_learn_classification(DOC *docs,
         (100.0*timing_profile.time_select)/(float)(runtime_end-runtime_start));
     }
     else {
-      printf("Runtime in cpu-seconds: %.2f\n",
+      printf(" + Runtime in cpu-seconds: %.2f\n",
 	     (runtime_end-runtime_start)/100.0);
     }
 
@@ -233,7 +234,7 @@ void svm_learn_classification(DOC *docs,
       for(i=0;i<totdoc;i++) 
 	if(inconsistent[i]) 
 	  inconsistentnum++;
-      printf("Number of SV: %ld (plus %ld inconsistent examples)\n",
+      printf(" + Number of SV: %ld (plus %ld inconsistent examples)\n",
 	     model->sv_num-1,inconsistentnum);
     }
     else {
@@ -244,7 +245,7 @@ void svm_learn_classification(DOC *docs,
 	    learn_parm->epsilon_a)) 
 	  upsupvecnum++;
       }
-      printf("Number of SV: %ld (including %ld at upper bound)\n",
+      printf(" + Number of SV: %ld (including %ld at upper bound)\n",
 	     model->sv_num-1,upsupvecnum);
     }
     
@@ -257,33 +258,33 @@ void svm_learn_classification(DOC *docs,
 	model_length+=a[i]*label[i]*lin[i];
       }
       model_length=sqrt(model_length);
-      fprintf(stdout,"L1 loss: loss=%.5f\n",loss);
-      fprintf(stdout,"Norm of weight vector: |w|=%.5f\n",model_length);
+      fprintf(stdout," + L1 loss: loss=%.5f\n",loss);
+      fprintf(stdout," + Norm of weight vector: |w|=%.5f\n",model_length);
       example_length=estimate_sphere(model,kernel_parm); 
-      fprintf(stdout,"Norm of longest example vector: |x|=%.5f\n",
+      fprintf(stdout," + Norm of longest example vector: |x|=%.5f\n",
 	      length_of_longest_document_vector(docs,totdoc,kernel_parm));
-      fprintf(stdout,"Estimated VCdim of classifier: VCdim<=%.5f\n",
+      fprintf(stdout," + Estimated VCdim of classifier: VCdim<=%.5f\n",
 	      estimate_margin_vcdim(model,model_length,example_length,
 				    kernel_parm));
       if((!learn_parm->remove_inconsistent) && (!transduction)) {
 	runtime_start_xa=get_runtime();
 	if(verbosity>=1) {
-	  printf("Computing XiAlpha-estimates..."); fflush(stdout);
+	  printf(" + Computing XiAlpha-estimates..."); fflush(stdout);
 	}
 	compute_xa_estimates(model,label,unlabeled,totdoc,docs,lin,a,
 			     kernel_parm,learn_parm,&(model->xa_error),
 			     &(model->xa_recall),&(model->xa_precision));
 	if(verbosity>=1) {
-	  printf("done\n");
+	  printf("done \n");
 	}
-	printf("Runtime for XiAlpha-estimates in cpu-seconds: %.2f\n",
+	printf(" + Runtime for XiAlpha-estimates in cpu-seconds: %.2f\n",
 	       (get_runtime()-runtime_start_xa)/100.0);
 	
-	fprintf(stdout,"XiAlpha-estimate of the error: error<=%.2f%% (rho=%.2f,depth=%ld)\n",
+	fprintf(stdout," + XiAlpha-estimate of the error: error<=%.2f%% (rho=%.2f,depth=%ld)\n",
 		model->xa_error,learn_parm->rho,learn_parm->xa_depth);
-	fprintf(stdout,"XiAlpha-estimate of the recall: recall=>%.2f%% (rho=%.2f,depth=%ld)\n",
+	fprintf(stdout," + XiAlpha-estimate of the recall: recall=>%.2f%% (rho=%.2f,depth=%ld)\n",
 		model->xa_recall,learn_parm->rho,learn_parm->xa_depth);
-	fprintf(stdout,"XiAlpha-estimate of the precision: precision=>%.2f%% (rho=%.2f,depth=%ld)\n",
+	fprintf(stdout," + XiAlpha-estimate of the precision: precision=>%.2f%% (rho=%.2f,depth=%ld)\n",
 		model->xa_precision,learn_parm->rho,learn_parm->xa_depth);
       }
       else if(!learn_parm->remove_inconsistent) {
@@ -291,7 +292,7 @@ void svm_learn_classification(DOC *docs,
       }
     }
     if(verbosity>=1) {
-      printf("Number of kernel evaluations: %ld\n",kernel_cache_statistic);
+      printf(" + Number of kernel evaluations: %ld\n",kernel_cache_statistic);
     }
   }
 
@@ -306,7 +307,7 @@ void svm_learn_classification(DOC *docs,
       a_fullset[i]=a[i];
     }
     if(verbosity>=1) {
-      printf("Computing leave-one-out");
+      printf(" + Computing leave-one-out");
     }
     
     /* repeat this loop for every held-out example */
@@ -315,7 +316,7 @@ void svm_learn_classification(DOC *docs,
 	 < 1.0) { 
 	/* guaranteed to not produce a leave-one-out error */
 	if(verbosity==1) {
-	  printf("+"); fflush(stdout); 
+	  printf(" + +"); fflush(stdout); 
 	}
       }
       else if(xi_fullset[heldout] > 1.0) {
@@ -323,7 +324,7 @@ void svm_learn_classification(DOC *docs,
 	loo_count++;
 	if(label[heldout] > 0)  loo_count_pos++; else loo_count_neg++;
 	if(verbosity==1) {
-	  printf("-"); fflush(stdout); 
+	  printf(" + -"); fflush(stdout); 
 	}
       }
       else {
@@ -334,9 +335,9 @@ void svm_learn_classification(DOC *docs,
 	/* shrunk away. Assumes that lin is up to date! */
 	shrink_state.active[heldout]=1;  
 	if(verbosity>=2) 
-	  printf("\nLeave-One-Out test on example %ld\n",heldout);
+	  printf("\n++ Leave-One-Out test on example %ld\n",heldout);
 	if(verbosity>=1) {
-	  printf("(?[%ld]",heldout); fflush(stdout); 
+	  printf(" + (?[%ld]",heldout); fflush(stdout); 
 	}
 	
 	optimize_to_convergence(docs,label,totdoc,totwords,learn_parm,
@@ -345,18 +346,18 @@ void svm_learn_classification(DOC *docs,
 				a,lin,c,&timing_profile,
 				&maxdiff,heldout,(long)2);
 
-	/* printf("%.20f\n",(lin[heldout]-model->b)*(double)label[heldout]); */
+	/* printf(" + %.20f\n",(lin[heldout]-model->b)*(double)label[heldout]); */
 
 	if(((lin[heldout]-model->b)*(double)label[heldout]) <= 0.0) { 
 	  loo_count++;                            /* there was a loo-error */
 	  if(label[heldout] > 0)  loo_count_pos++; else loo_count_neg++;
 	  if(verbosity>=1) {
-	    printf("-)"); fflush(stdout); 
+	    printf(" + -)"); fflush(stdout); 
 	  }
 	}
 	else {
 	  if(verbosity>=1) {
-	    printf("+)"); fflush(stdout); 
+	    printf(" + +)"); fflush(stdout); 
 	  }
 	}
 	/* now we need to restore the original data set*/
@@ -374,7 +375,7 @@ void svm_learn_classification(DOC *docs,
 			    a,lin,c,&timing_profile,
 			    &maxdiff,(long)-1,(long)1);
     if(verbosity >= 1) 
-      printf("done.\n");
+      printf("done .\n");
     
     
     /* after all leave-one-out computed */
@@ -383,15 +384,15 @@ void svm_learn_classification(DOC *docs,
     model->loo_precision=(trainpos-loo_count_pos)/
       (double)(trainpos-loo_count_pos+loo_count_neg)*100.0;
     if(verbosity >= 1) {
-      fprintf(stdout,"Leave-one-out estimate of the error: error=%.2f%%\n",
+      fprintf(stdout," + Leave-one-out estimate of the error: error=%.2f%%\n",
 	      model->loo_error);
-      fprintf(stdout,"Leave-one-out estimate of the recall: recall=%.2f%%\n",
+      fprintf(stdout," + Leave-one-out estimate of the recall: recall=%.2f%%\n",
 	      model->loo_recall);
-      fprintf(stdout,"Leave-one-out estimate of the precision: precision=%.2f%%\n",
+      fprintf(stdout," + Leave-one-out estimate of the precision: precision=%.2f%%\n",
 	      model->loo_precision);
-      fprintf(stdout,"Actual leave-one-outs computed:  %ld (rho=%.2f)\n",
+      fprintf(stdout," + Actual leave-one-outs computed:  %ld (rho=%.2f)\n",
 	      loocomputed,learn_parm->rho);
-      printf("Runtime for leave-one-out in cpu-seconds: %.2f\n",
+      printf(" + Runtime for leave-one-out in cpu-seconds: %.2f\n",
 	     (double)(get_runtime()-runtime_start_loo)/100.0);
     }
   }
@@ -518,7 +519,7 @@ void svm_learn_regression(DOC *docs, dou
   if(learn_parm->svm_c == 0.0) {  /* default value for C */
     learn_parm->svm_c=1.0/(r_delta_avg*r_delta_avg);
     if(verbosity>=1) 
-      printf("Setting default regularization parameter C=%.4f\n",
+      printf(" + Setting default regularization parameter C=%.4f\n",
 	     learn_parm->svm_c);
   }
 
@@ -542,7 +543,7 @@ void svm_learn_regression(DOC *docs, dou
   } 
 
   if(verbosity==1) {
-    printf("Optimizing"); fflush(stdout);
+    printf(" + Optimizing"); fflush(stdout);
   }
 
   /* train the svm */
@@ -553,13 +554,13 @@ void svm_learn_regression(DOC *docs, dou
 				     (long)1);
   
   if(verbosity>=1) {
-    if(verbosity==1) printf("done. (%ld iterations)\n",iterations);
+    if(verbosity==1) printf("done . (%ld iterations)\n",iterations);
 
-    printf("Optimization finished (maxdiff=%.5f).\n",maxdiff); 
+    printf(" + Optimization finished (maxdiff=%.5f).\n",maxdiff); 
 
     runtime_end=get_runtime();
     if(verbosity>=2) {
-      printf("Runtime in cpu-seconds: %.2f (%.2f%% for kernel/%.2f%% for optimizer/%.2f%% for final/%.2f%% for update/%.2f%% for model/%.2f%% for check/%.2f%% for select)\n",
+      printf(" + Runtime in cpu-seconds: %.2f (%.2f%% for kernel/%.2f%% for optimizer/%.2f%% for final/%.2f%% for update/%.2f%% for model/%.2f%% for check/%.2f%% for select)\n",
         ((float)runtime_end-(float)runtime_start)/100.0,
         (100.0*timing_profile.time_kernel)/(float)(runtime_end-runtime_start),
 	(100.0*timing_profile.time_opti)/(float)(runtime_end-runtime_start),
@@ -570,7 +571,7 @@ void svm_learn_regression(DOC *docs, dou
         (100.0*timing_profile.time_select)/(float)(runtime_end-runtime_start));
     }
     else {
-      printf("Runtime in cpu-seconds: %.2f\n",
+      printf(" + Runtime in cpu-seconds: %.2f\n",
 	     (runtime_end-runtime_start)/100.0);
     }
 
@@ -579,7 +580,7 @@ void svm_learn_regression(DOC *docs, dou
       for(i=0;i<totdoc;i++) 
 	if(inconsistent[i]) 
 	  inconsistentnum++;
-      printf("Number of SV: %ld (plus %ld inconsistent examples)\n",
+      printf(" + Number of SV: %ld (plus %ld inconsistent examples)\n",
 	     model->sv_num-1,inconsistentnum);
     }
     else {
@@ -590,7 +591,7 @@ void svm_learn_regression(DOC *docs, dou
 	    learn_parm->epsilon_a)) 
 	  upsupvecnum++;
       }
-      printf("Number of SV: %ld (including %ld at upper bound)\n",
+      printf(" + Number of SV: %ld (including %ld at upper bound)\n",
 	     model->sv_num-1,upsupvecnum);
     }
     
@@ -603,14 +604,14 @@ void svm_learn_regression(DOC *docs, dou
 	model_length+=a[i]*label[i]*lin[i];
       }
       model_length=sqrt(model_length);
-      fprintf(stdout,"L1 loss: loss=%.5f\n",loss);
-      fprintf(stdout,"Norm of weight vector: |w|=%.5f\n",model_length);
+      fprintf(stdout," + L1 loss: loss=%.5f\n",loss);
+      fprintf(stdout," + Norm of weight vector: |w|=%.5f\n",model_length);
       example_length=estimate_sphere(model,kernel_parm); 
-      fprintf(stdout,"Norm of longest example vector: |x|=%.5f\n",
+      fprintf(stdout," + Norm of longest example vector: |x|=%.5f\n",
 	      length_of_longest_document_vector(docs,totdoc,kernel_parm));
     }
     if(verbosity>=1) {
-      printf("Number of kernel evaluations: %ld\n",kernel_cache_statistic);
+      printf(" + Number of kernel evaluations: %ld\n",kernel_cache_statistic);
     }
   }
     
@@ -660,8 +661,8 @@ void svm_learn_ranking(DOC *docs, double
   MODEL pairmodel;
 
   if(kernel_parm->kernel_type != LINEAR) {
-    printf("Learning rankings is not implemented for non-linear kernels in this version!\n");
-    printf("WARNING: Using linear kernel instead!\n");
+    printf(" + Learning rankings is not implemented for non-linear kernels in this version!\n");
+    printf(" + WARNING: Using linear kernel instead!\n");
     kernel_parm->kernel_type = LINEAR;
   }
 
@@ -674,7 +675,7 @@ void svm_learn_ranking(DOC *docs, double
     }
   }
 
-  printf("Constructing %ld rank constraints...",totpair); fflush(stdout);
+  printf(" + Constructing %ld rank constraints...",totpair); fflush(stdout);
   docdiff=(DOC *)my_malloc(sizeof(DOC)*totpair);
   target=(double *)my_malloc(sizeof(double)*totpair); 
   greater=(long *)my_malloc(sizeof(long)*totpair); 
@@ -707,7 +708,7 @@ void svm_learn_ranking(DOC *docs, double
       }
     }
   }
-  printf("done.\n"); fflush(stdout);
+  printf("done .\n"); fflush(stdout);
 
   /* must use unbiased hyperplane on difference vectors */
   learn_parm->biased_hyperplane=0;
@@ -875,8 +876,7 @@ long optimize_to_convergence(DOC *docs, 
     if(kernel_cache)
       kernel_cache->time=iteration;  /* for lru cache */
     if(verbosity>=2) {
-      printf(
-	"Iteration %ld: ",iteration); fflush(stdout);
+      printf(" + Iteration %ld: ",iteration); fflush(stdout);
     }
     else if(verbosity==1) {
       printf("."); fflush(stdout);
@@ -884,7 +884,7 @@ long optimize_to_convergence(DOC *docs, 
 
     if(verbosity>=2) t0=get_runtime();
     if(verbosity>=3) {
-      printf("\nSelecting working set... "); fflush(stdout); 
+      printf("\n++ Selecting working set... "); fflush(stdout); 
     }
 
     if(learn_parm->svm_newvarsinqp>learn_parm->svm_maxqpsize) 
@@ -981,7 +981,7 @@ long optimize_to_convergence(DOC *docs, 
     }
 
     if(verbosity>=2) {
-      printf(" %ld vectors chosen\n",choosenum); fflush(stdout); 
+      printf(" +  %ld vectors chosen\n",choosenum); fflush(stdout); 
     }
 
     if(verbosity>=2) t1=get_runtime();
@@ -1013,7 +1013,7 @@ long optimize_to_convergence(DOC *docs, 
     if(verbosity>=3) {
       criterion=compute_objective_function(a,lin,c,learn_parm->eps,label,
 		                           active2dnum);
-      printf("Objective function (over active variables): %.16f\n",criterion);
+      printf(" + Objective function (over active variables): %.16f\n",criterion);
       fflush(stdout); 
     }
 
@@ -1054,7 +1054,7 @@ long optimize_to_convergence(DOC *docs, 
 	if(verbosity==1) {
 	  printf("\n");
 	}
-	printf(" Checking optimality of inactive variables..."); 
+	printf(" +  Checking optimality of inactive variables..."); 
 	fflush(stdout);
       }
       t1=get_runtime();
@@ -1073,8 +1073,8 @@ long optimize_to_convergence(DOC *docs, 
       timing_profile->time_shrink+=get_runtime()-t1;
       if(((verbosity>=1) && (kernel_parm->kernel_type != LINEAR)) 
 	 || (verbosity>=2)) {
-	printf("done.\n");  fflush(stdout);
-        printf(" Number of inactive variables = %ld\n",inactivenum);
+	printf("done .\n");  fflush(stdout);
+        printf(" +  Number of inactive variables = %ld\n",inactivenum);
       }		  
     }
 
@@ -1089,7 +1089,7 @@ long optimize_to_convergence(DOC *docs, 
       learn_parm->epsilon_crit=epsilon_crit_org;
     
     if(verbosity>=2) {
-      printf(" => (%ld SV (incl. %ld SV at u-bound), max violation=%.5f)\n",
+      printf(" +  => (%ld SV (incl. %ld SV at u-bound), max violation=%.5f)\n",
 	     supvecnum,model->at_upper_bound,(*maxdiff)); 
       fflush(stdout);
     }
@@ -1103,7 +1103,7 @@ long optimize_to_convergence(DOC *docs, 
       }
       activenum=compute_index(shrink_state->active,totdoc,active2dnum);
       inactivenum=0;
-      if(verbosity==1) printf("done\n");
+      if(verbosity==1) printf("done \n");
       retrain=incorporate_unlabeled_examples(model,label,inconsistent,
 					     unlabeled,a,lin,totdoc,
 					     selcrit,selexam,key,
@@ -1133,7 +1133,7 @@ long optimize_to_convergence(DOC *docs, 
 
     if((!retrain) && learn_parm->remove_inconsistent) {
       if(verbosity>=1) {
-	printf(" Moving training errors to inconsistent examples...");
+	printf(" +  Moving training errors to inconsistent examples...");
 	fflush(stdout);
       }
       if(learn_parm->remove_inconsistent == 1) {
@@ -1154,9 +1154,9 @@ long optimize_to_convergence(DOC *docs, 
 	} 
       }
       if(verbosity>=1) {
-	printf("done.\n");
+	printf("done .\n");
 	if(retrain) {
-	  printf(" Now %ld inconsistent examples.\n",inconsistentnum);
+	  printf(" +  Now %ld inconsistent examples.\n",inconsistentnum);
 	}
       }
     }
@@ -1253,7 +1253,7 @@ void optimize_svm(DOC *docs, long int *l
 				      qp);
 
     if(verbosity>=3) {
-      printf("Running optimizer..."); fflush(stdout);
+      printf(" + Running optimizer..."); fflush(stdout);
     }
     /* call the qp-subsolver */
     a_v=optimize_qp(qp,epsilon_crit_target,
@@ -1263,7 +1263,7 @@ void optimize_svm(DOC *docs, long int *l
                                    /* b is calculated in calculate_model. */
 		    learn_parm);
     if(verbosity>=3) {         
-      printf("done\n");
+      printf("done \n");
     }
 
     for(i=0;i<varnum;i++) {
@@ -1289,7 +1289,7 @@ void compute_matrices_for_optimization(D
   register double kernel_temp;
 
   if(verbosity>=3) {
-    fprintf(stdout,"Computing qp-matrices (type %ld kernel [degree %ld, rbf_gamma %f, coef_lin %f, coef_const %f])...",kernel_parm->kernel_type,kernel_parm->poly_degree,kernel_parm->rbf_gamma,kernel_parm->coef_lin,kernel_parm->coef_const); 
+    fprintf(stdout," + Computing qp-matrices (type %ld kernel [degree %ld, rbf_gamma %f, coef_lin %f, coef_const %f])...",kernel_parm->kernel_type,kernel_parm->poly_degree,kernel_parm->rbf_gamma,kernel_parm->coef_lin,kernel_parm->coef_const); 
     fflush(stdout);
   }
 
@@ -1336,7 +1336,7 @@ void compute_matrices_for_optimization(D
 
     if(verbosity>=3) {
       if(i % 20 == 0) {
-	fprintf(stdout,"%ld..",i); fflush(stdout);
+	fprintf(stdout," + %ld..",i); fflush(stdout);
       }
     }
   }
@@ -1364,7 +1364,7 @@ long calculate_svm_model(DOC *docs, long
   double ex_c,b_temp,b_low,b_high;
 
   if(verbosity>=3) {
-    printf("Calculating model..."); fflush(stdout);
+    printf(" + Calculating model..."); fflush(stdout);
   }
 
   if(!learn_parm->biased_hyperplane) {
@@ -1462,12 +1462,12 @@ long calculate_svm_model(DOC *docs, long
     }
     else {
       model->b=-(b_high+b_low)/2.0;  /* select b as the middle of range */
-      /* printf("\nb_low=%lf, b_high=%lf,b=%lf\n",b_low,b_high,model->b); */
+      /* printf("\n++ b_low=%lf, b_high=%lf,b=%lf\n",b_low,b_high,model->b); */
     }
   }
 
   if(verbosity>=3) {
-    printf("done\n"); fflush(stdout);
+    printf("done \n"); fflush(stdout);
   }
 
   return(model->sv_num-1); /* have to substract one, since element 0 is empty*/
@@ -1557,7 +1557,7 @@ long identify_inconsistent(double *a, lo
 	inconsistent[i]=1;  /* never choose again */
 	retrain=2;          /* start over */
 	if(verbosity>=3) {
-	  printf("inconsistent(%ld)..",i); fflush(stdout);
+	  printf(" + inconsistent(%ld)..",i); fflush(stdout);
 	}
     }
   }
@@ -1585,7 +1585,7 @@ long identify_misclassified(double *lin,
 	inconsistent[i]=1;  /* never choose again */
 	retrain=2;          /* start over */
 	if(verbosity>=3) {
-	  printf("inconsistent(%ld)..",i); fflush(stdout);
+	  printf(" + inconsistent(%ld)..",i); fflush(stdout);
 	}
     }
   }
@@ -1621,7 +1621,7 @@ long identify_one_misclassified(double *
     inconsistent[maxex]=1;  /* never choose again */
     retrain=2;          /* start over */
     if(verbosity>=3) {
-      printf("inconsistent(%ld)..",i); fflush(stdout);
+      printf(" + inconsistent(%ld)..",i); fflush(stdout);
     }
   }
   return(retrain);
@@ -1760,13 +1760,13 @@ long incorporate_unlabeled_examples(MODE
       }
     }
     if((!unlabeled[i]) && (a[i]>(learn_parm->svm_cost[i]-learn_parm->epsilon_a))) {
-      /*      printf("Ubounded %ld (class %ld, unlabeled %ld)\n",i,label[i],unlabeled[i]); */
+      /*      printf(" + Ubounded %ld (class %ld, unlabeled %ld)\n",i,label[i],unlabeled[i]); */
     }
   }
   if(verbosity>=2) {
-    printf("POS=%ld, ORGPOS=%ld, ORGNEG=%ld\n",pos,orgpos,orgneg);
-    printf("POS=%ld, NEWPOS=%ld, NEWNEG=%ld\n",pos,newpos,newneg);
-    printf("pos ratio = %f (%f).\n",(double)(upos)/(double)(allunlab),posratio);
+    printf(" + POS=%ld, ORGPOS=%ld, ORGNEG=%ld\n",pos,orgpos,orgneg);
+    printf(" + POS=%ld, NEWPOS=%ld, NEWNEG=%ld\n",pos,newpos,newneg);
+    printf(" + pos ratio = %f (%f).\n",(double)(upos)/(double)(allunlab),posratio);
     fflush(stdout);
   }
 
@@ -1811,21 +1811,21 @@ long incorporate_unlabeled_examples(MODE
       }
     }
     if(verbosity>=1) {
-      /* printf("costratio %lf, costratio_unlab %lf, unlabbound %lf\n",
+      /* printf(" + costratio %lf, costratio_unlab %lf, unlabbound %lf\n",
 	 learn_parm->svm_costratio,learn_parm->svm_costratio_unlab,
 	 learn_parm->svm_unlabbound); */
-      printf("Classifying unlabeled data as %ld POS / %ld NEG.\n",
+      printf(" + Classifying unlabeled data as %ld POS / %ld NEG.\n",
 	     unsupaddnum1,unsupaddnum2); 
       fflush(stdout);
     }
     if(verbosity >= 1) 
-      printf("Retraining.");
+      printf(" + Retraining.");
     if(verbosity >= 2) printf("\n");
     return((long)3);
   }
   if((transductcycle % check_every) == 0) {
     if(verbosity >= 1) 
-      printf("Retraining.");
+      printf(" + Retraining.");
     if(verbosity >= 2) printf("\n");
     j1=0;
     j2=0;
@@ -1859,10 +1859,10 @@ long incorporate_unlabeled_examples(MODE
     }
 
     if(verbosity>=2) {
-      /* printf("costratio %lf, costratio_unlab %lf, unlabbound %lf\n",
+      /* printf(" + costratio %lf, costratio_unlab %lf, unlabbound %lf\n",
 	     learn_parm->svm_costratio,learn_parm->svm_costratio_unlab,
 	     learn_parm->svm_unlabbound); */
-      printf("%ld positive -> Added %ld POS / %ld NEG unlabeled examples.\n",
+      printf(" + %ld positive -> Added %ld POS / %ld NEG unlabeled examples.\n",
 	     upos,unsupaddnum1,unsupaddnum2); 
       fflush(stdout);
     }
@@ -1890,7 +1890,7 @@ long incorporate_unlabeled_examples(MODE
     }
     model_length=sqrt(model_length); 
     if(verbosity>=2) {
-      printf("Model-length = %f (%f), loss = %f, objective = %f\n",
+      printf(" + Model-length = %f (%f), loss = %f, objective = %f\n",
 	     model_length,sumalpha,loss,loss+0.5*model_length*model_length);
       fflush(stdout);
     }
@@ -1948,7 +1948,7 @@ long incorporate_unlabeled_examples(MODE
     switchnum+=unsupaddnum1+unsupaddnum2;
 
     /* stop and print out current margin
-       printf("switchnum %ld %ld\n",switchnum,kernel_parm->poly_degree);
+       printf(" + switchnum %ld %ld\n",switchnum,kernel_parm->poly_degree);
        if(switchnum == 2*kernel_parm->poly_degree) {
        learn_parm->svm_unlabbound=1;
        }
@@ -1963,7 +1963,7 @@ long incorporate_unlabeled_examples(MODE
 	write_prediction(learn_parm->predfile,model,lin,a,unlabeled,label,
 			 totdoc,learn_parm);  
 	if(verbosity>=1)
-	  printf("Number of switches: %ld\n",switchnum);
+	  printf(" + Number of switches: %ld\n",switchnum);
 	return((long)0);
       }
       switchsens=switchsensorg;
@@ -1973,11 +1973,11 @@ long incorporate_unlabeled_examples(MODE
       }
       model->at_upper_bound=0; /* since upper bound increased */
       if(verbosity>=1) 
-	printf("Increasing influence of unlabeled examples to %f%% .",
+	printf(" + Increasing influence of unlabeled examples to %f%% .",
 	       learn_parm->svm_unlabbound*100.0);
     }
     else if(verbosity>=1) {
-      printf("%ld positive -> Switching labels of %ld POS / %ld NEG unlabeled examples.",
+      printf(" + %ld positive -> Switching labels of %ld POS / %ld NEG unlabeled examples.",
 	     upos,unsupaddnum1,unsupaddnum2); 
       fflush(stdout);
     }
@@ -2340,7 +2340,7 @@ long shrink_problem(LEARN_PARM *learn_pa
     /* Shrink problem by removing those variables which are */
     /* optimal at a bound for a minimum number of iterations */
     if(verbosity>=2) {
-      printf(" Shrinking..."); fflush(stdout);
+      printf(" +  Shrinking..."); fflush(stdout);
     }
     if(kernel_parm->kernel_type != LINEAR) { /*  non-linear case save alphas */
       a_old=(double *)my_malloc(sizeof(double)*totdoc);
@@ -2364,8 +2364,8 @@ long shrink_problem(LEARN_PARM *learn_pa
       shrink_state->deactnum=0;
     }
     if(verbosity>=2) {
-      printf("done.\n"); fflush(stdout);
-      printf(" Number of inactive variables = %ld\n",totdoc-activenum);
+      printf("done .\n"); fflush(stdout);
+      printf(" +  Number of inactive variables = %ld\n",totdoc-activenum);
     }
   }
   return(activenum);
@@ -2423,7 +2423,7 @@ void reactivate_inactive_examples(long i
     inactive2dnum=(long *)my_malloc(sizeof(long)*(totdoc+11));
     for(t=shrink_state->deactnum-1;(t>=0) && shrink_state->a_history[t];t--) {
       if(verbosity>=2) {
-	printf("%ld..",t); fflush(stdout);
+	printf(" + %ld..",t); fflush(stdout);
       }
       a_old=shrink_state->a_history[t];    
       for(i=0;i<totdoc;i++) {
@@ -2576,7 +2576,7 @@ void kernel_cache_shrink(KERNEL_CACHE *k
   long *keep;
 
   if(verbosity>=2) {
-    printf(" Reorganizing cache..."); fflush(stdout);
+    printf(" +  Reorganizing cache..."); fflush(stdout);
   }
 
   keep=(long *)my_malloc(sizeof(long)*totdoc);
@@ -2626,8 +2626,8 @@ void kernel_cache_shrink(KERNEL_CACHE *k
   free(keep);
 
   if(verbosity>=2) {
-    printf("done.\n"); fflush(stdout);
-    printf(" Cache-size in rows = %ld\n",kernel_cache->max_elems);
+    printf("done .\n"); fflush(stdout);
+    printf(" +  Cache-size in rows = %ld\n",kernel_cache->max_elems);
   }
 }
 
@@ -2652,8 +2652,8 @@ void kernel_cache_init(KERNEL_CACHE *ker
   }
 
   if(verbosity>=2) {
-    printf(" Cache-size in rows = %ld\n",kernel_cache->max_elems);
-    printf(" Kernel evals so far: %ld\n",kernel_cache_statistic);    
+    printf(" +  Cache-size in rows = %ld\n",kernel_cache->max_elems);
+    printf(" +  Kernel evals so far: %ld\n",kernel_cache_statistic);    
   }
 
   kernel_cache->elems=0;   /* initialize cache */
@@ -2928,7 +2928,7 @@ double distribute_alpha_t_greedily(long 
 	allskip=0;
 	if(val < thresh) {
 	  i=svnum;
-	  /*	  printf("EARLY"); */
+	  /*	  printf(" + EARLY"); */
 	}
       }
     }
@@ -2951,7 +2951,7 @@ double distribute_alpha_t_greedily(long 
   free(cache);
   free(trow);
 
-  /*  printf("Distribute[%ld](%ld)=%f, ",docnum,best_depth,best); */
+  /*  printf(" + Distribute[%ld](%ld)=%f, ",docnum,best_depth,best); */
   return(best);
 }
 
@@ -3015,11 +3015,11 @@ void estimate_transduction_quality(MODEL
       }
     }
   }
-  printf("xacrit>=1: labeledpos=%.5f labeledneg=%.5f default=%.5f\n",(double)labpos/(double)totlab*100.0,(double)labneg/(double)totlab*100.0,(double)totlabpos/(double)(totlab)*100.0);
-  printf("xacrit>=1: unlabelpos=%.5f unlabelneg=%.5f\n",(double)ulabpos/(double)totulab*100.0,(double)ulabneg/(double)totulab*100.0);
-  printf("xacrit>=1: labeled=%.5f unlabled=%.5f all=%.5f\n",(double)lab/(double)totlab*100.0,(double)ulab/(double)totulab*100.0,(double)l/(double)(totdoc)*100.0);
-  printf("xacritsum: labeled=%.5f unlabled=%.5f all=%.5f\n",(double)labsum/(double)totlab*100.0,(double)ulabsum/(double)totulab*100.0,(double)(labsum+ulabsum)/(double)(totdoc)*100.0);
-  printf("r_delta_sq=%.5f xisum=%.5f asum=%.5f\n",r_delta_sq,xisum,asum);
+  printf(" + xacrit>=1: labeledpos=%.5f labeledneg=%.5f default=%.5f\n",(double)labpos/(double)totlab*100.0,(double)labneg/(double)totlab*100.0,(double)totlabpos/(double)(totlab)*100.0);
+  printf(" + xacrit>=1: unlabelpos=%.5f unlabelneg=%.5f\n",(double)ulabpos/(double)totulab*100.0,(double)ulabneg/(double)totulab*100.0);
+  printf(" + xacrit>=1: labeled=%.5f unlabled=%.5f all=%.5f\n",(double)lab/(double)totlab*100.0,(double)ulab/(double)totulab*100.0,(double)l/(double)(totdoc)*100.0);
+  printf(" + xacritsum: labeled=%.5f unlabled=%.5f all=%.5f\n",(double)labsum/(double)totlab*100.0,(double)ulabsum/(double)totulab*100.0,(double)(labsum+ulabsum)/(double)(totdoc)*100.0);
+  printf(" + r_delta_sq=%.5f xisum=%.5f asum=%.5f\n",r_delta_sq,xisum,asum);
 }
 
 double estimate_margin_vcdim(MODEL *model, double w, double R, 
@@ -3140,11 +3140,11 @@ void write_model(char *modelfile, MODEL 
   long j,i;
 
   if(verbosity>=1) {
-    printf("Writing model file..."); fflush(stdout);
+    printf(" + Writing model file..."); fflush(stdout);
   }
   if ((modelfl = fopen (modelfile, "w")) == NULL)
   { perror (modelfile); exit (1); }
-  fprintf(modelfl,"SVM-light Version %s\n",VERSION);
+  fprintf(modelfl,"SVM-light Version %s\n",VERSION_SVMLIGHT);
   fprintf(modelfl,"%ld # kernel type\n",
 	  model->kernel_parm.kernel_type);
   fprintf(modelfl,"%ld # kernel parameter -d \n",
@@ -3173,7 +3173,7 @@ void write_model(char *modelfile, MODEL 
   }
   fclose(modelfl);
   if(verbosity>=1) {
-    printf("done\n");
+    printf("done \n");
   }
 }
 
@@ -3188,7 +3188,7 @@ void write_prediction(char *predfile, MO
   double dist,a_max;
 
   if(verbosity>=1) {
-    printf("Writing prediction file..."); fflush(stdout);
+    printf(" + Writing prediction file..."); fflush(stdout);
   }
   if ((predfl = fopen (predfile, "w")) == NULL)
   { perror (predfile); exit (1); }
@@ -3216,7 +3216,7 @@ void write_prediction(char *predfile, MO
   }
   fclose(predfl);
   if(verbosity>=1) {
-    printf("done\n");
+    printf("done \n");
   }
 }
 
@@ -3227,16 +3227,17 @@ void write_alphas(char *alphafile, doubl
   long i;
 
   if(verbosity>=1) {
-    printf("Writing alpha file..."); fflush(stdout);
+    printf(" + Writing alpha file..."); fflush(stdout);
   }
   if ((alphafl = fopen (alphafile, "w")) == NULL)
   { perror (alphafile); exit (1); }
   for(i=0;i<totdoc;i++) {
     fprintf(alphafl,"%.8g\n",a[i]*(double)label[i]);
+    //fprintf(alphafl,"%.60e\n",a[i]*(double)label[i]);
   }
   fclose(alphafl);
   if(verbosity>=1) {
-    printf("done\n");
+    printf("done \n");
   }
 }
 
diff -rupN orig/svm_learn_main.c svml/svm_learn_main.c
--- orig/svm_learn_main.c	2011-08-26 16:51:19.000000000 -0400
+++ svml/svm_learn_main.c	2011-08-26 17:22:03.000000000 -0400
@@ -26,8 +26,9 @@
 
 char docfile[200];           /* file with training examples */
 char modelfile[200];         /* file for resulting classifier */
+char restartfile[200];       /* file with initial alphas */
 
-void   read_input_parameters(int, char **, char *, char *,long *, long *, 
+void   read_input_parameters(int, char **, char *, char *, char *, long *, 
 			     LEARN_PARM *, KERNEL_PARM *);
 void   wait_any_key();
 void   print_help();
@@ -36,84 +37,61 @@ void   print_help();
 
 int main (int argc, char* argv[])
 {  
-  DOC *docs;  /* training examples */
-  long max_docs,max_words_doc;
-  long totwords,totdoc,ll,i;
-  long kernel_cache_size;
+  DOC **docs;  /* training examples */
+  long totwords,totdoc,i;
   double *target;
-  KERNEL_CACHE kernel_cache;
+  double *alpha_in=NULL;
+  KERNEL_CACHE *kernel_cache;
   LEARN_PARM learn_parm;
   KERNEL_PARM kernel_parm;
-  MODEL model;
+  MODEL *model=(MODEL *)my_malloc(sizeof(MODEL));
 
-  read_input_parameters(argc,argv,docfile,modelfile,&verbosity,
-			&kernel_cache_size,&learn_parm,&kernel_parm);
+  read_input_parameters(argc,argv,docfile,modelfile,restartfile,&verbosity,
+			&learn_parm,&kernel_parm);
+  read_documents(docfile,&docs,&target,&totwords,&totdoc);
+  if(restartfile[0]) alpha_in=read_alphas(restartfile,totdoc);
 
-  if(verbosity>=1) {
-    printf("Scanning examples..."); fflush(stdout);
+  if(kernel_parm.kernel_type == LINEAR) { /* don't need the cache */
+    kernel_cache=NULL;
   }
-  nol_ll(docfile,&max_docs,&max_words_doc,&ll); /* scan size of input file */
-  max_words_doc+=2;
-  ll+=2;
-  max_docs+=2;
-  if(verbosity>=1) {
-    printf("done\n"); fflush(stdout);
+  else {
+    /* Always get a new kernel cache. It is not possible to use the
+       same cache for two different training runs */
+    kernel_cache=kernel_cache_init(totdoc,learn_parm.kernel_cache_size);
   }
 
-  docs = (DOC *)my_malloc(sizeof(DOC)*max_docs);         /* feature vectors */
-  target = (double *)my_malloc(sizeof(double)*max_docs); /* target values */
-
-  read_documents(docfile,docs,target,max_words_doc,ll,&totwords,&totdoc);
-
-  if(kernel_parm.kernel_type == LINEAR) { /* don't need the cache */
-    if(learn_parm.type == CLASSIFICATION) {
-      svm_learn_classification(docs,target,totdoc,totwords,&learn_parm,
-			       &kernel_parm,NULL,&model);
-    }
-    else if(learn_parm.type == REGRESSION) {
-      svm_learn_regression(docs,target,totdoc,totwords,&learn_parm,
-			   &kernel_parm,NULL,&model);
-    }
-    else if(learn_parm.type == RANKING) {
-      svm_learn_ranking(docs,target,totdoc,totwords,&learn_parm,
-			&kernel_parm,NULL,&model);
-    }
+  if(learn_parm.type == CLASSIFICATION) {
+    svm_learn_classification(docs,target,totdoc,totwords,&learn_parm,
+			     &kernel_parm,kernel_cache,model,alpha_in);
   }
-  else {
-    if(learn_parm.type == CLASSIFICATION) {
-      /* Always get a new kernel cache. It is not possible to use the
-         same cache for two different training runs */
-      kernel_cache_init(&kernel_cache,totdoc,kernel_cache_size);
-      svm_learn_classification(docs,target,totdoc,totwords,&learn_parm,
-			       &kernel_parm,&kernel_cache,&model);
-      /* Free the memory used for the cache. */
-      kernel_cache_cleanup(&kernel_cache);
-    }
-    else if(learn_parm.type == REGRESSION) {
-      /* Always get a new kernel cache. It is not possible to use the
-         same cache for two different training runs */
-      kernel_cache_init(&kernel_cache,2*totdoc,kernel_cache_size);
-      svm_learn_regression(docs,target,totdoc,totwords,&learn_parm,
-			   &kernel_parm,&kernel_cache,&model);
-      /* Free the memory used for the cache. */
-      kernel_cache_cleanup(&kernel_cache);
-    }
-    else if(learn_parm.type == RANKING) {
-      printf("Learning rankings is not implemented for non-linear kernels in this version!\n");
-      exit(1);
-    }
+  else if(learn_parm.type == REGRESSION) {
+    svm_learn_regression(docs,target,totdoc,totwords,&learn_parm,
+			 &kernel_parm,&kernel_cache,model);
+  }
+  else if(learn_parm.type == RANKING) {
+    svm_learn_ranking(docs,target,totdoc,totwords,&learn_parm,
+		      &kernel_parm,&kernel_cache,model);
+  }
+  else if(learn_parm.type == OPTIMIZATION) {
+    svm_learn_optimization(docs,target,totdoc,totwords,&learn_parm,
+			   &kernel_parm,kernel_cache,model,alpha_in);
+  }
+
+  if(kernel_cache) {
+    /* Free the memory used for the cache. */
+    kernel_cache_cleanup(kernel_cache);
   }
 
   /* Warning: The model contains references to the original data 'docs'.
      If you want to free the original data, and only keep the model, you 
      have to make a deep copy of 'model'. */
-  write_model(modelfile,&model);
+  /* deep_copy_of_model=copy_model(model); */
+  write_model(modelfile,model);
 
-  free(model.supvec);
-  free(model.alpha);
-  free(model.index);
+  free(alpha_in);
+  free_model(model,0);
   for(i=0;i<totdoc;i++) 
-    free(docs[i].words);
+    free_example(docs[i],1);
   free(docs);
   free(target);
 
@@ -123,7 +101,7 @@ int main (int argc, char* argv[])
 /*---------------------------------------------------------------------------*/
 
 void read_input_parameters(int argc,char *argv[],char *docfile,char *modelfile,
-			   long *verbosity,long *kernel_cache_size,
+			   char *restartfile,long *verbosity,
 			   LEARN_PARM *learn_parm,KERNEL_PARM *kernel_parm)
 {
   long i;
@@ -133,14 +111,17 @@ void read_input_parameters(int argc,char
   strcpy (modelfile, "svm_model");
   strcpy (learn_parm->predfile, "trans_predictions");
   strcpy (learn_parm->alphafile, "");
+  strcpy (restartfile, "");
   (*verbosity)=1;
   learn_parm->biased_hyperplane=1;
+  learn_parm->sharedslack=0;
   learn_parm->remove_inconsistent=0;
   learn_parm->skip_final_opt_check=0;
   learn_parm->svm_maxqpsize=10;
   learn_parm->svm_newvarsinqp=0;
   learn_parm->svm_iter_to_shrink=-9999;
-  (*kernel_cache_size)=40;
+  learn_parm->maxiter=100000;
+  learn_parm->kernel_cache_size=40;
   learn_parm->svm_c=0.0;
   learn_parm->eps=0.1;
   learn_parm->transduction_posratio=-1.0;
@@ -171,8 +152,9 @@ void read_input_parameters(int argc,char
       case 'f': i++; learn_parm->skip_final_opt_check=!atol(argv[i]); break;
       case 'q': i++; learn_parm->svm_maxqpsize=atol(argv[i]); break;
       case 'n': i++; learn_parm->svm_newvarsinqp=atol(argv[i]); break;
+      case '#': i++; learn_parm->maxiter=atol(argv[i]); break;
       case 'h': i++; learn_parm->svm_iter_to_shrink=atol(argv[i]); break;
-      case 'm': i++; (*kernel_cache_size)=atol(argv[i]); break;
+      case 'm': i++; learn_parm->kernel_cache_size=atol(argv[i]); break;
       case 'c': i++; learn_parm->svm_c=atof(argv[i]); break;
       case 'w': i++; learn_parm->eps=atof(argv[i]); break;
       case 'p': i++; learn_parm->transduction_posratio=atof(argv[i]); break;
@@ -189,6 +171,7 @@ void read_input_parameters(int argc,char
       case 'u': i++; strcpy(kernel_parm->custom,argv[i]); break;
       case 'l': i++; strcpy(learn_parm->predfile,argv[i]); break;
       case 'a': i++; strcpy(learn_parm->alphafile,argv[i]); break;
+      case 'y': i++; strcpy(restartfile,argv[i]); break;
       default: printf("\nUnrecognized option %s!\n\n",argv[i]);
 	       print_help();
 	       exit(0);
@@ -219,6 +202,13 @@ void read_input_parameters(int argc,char
   else if(strcmp(type,"p")==0) {
     learn_parm->type=RANKING;
   }
+  else if(strcmp(type,"o")==0) {
+    learn_parm->type=OPTIMIZATION;
+  }
+  else if(strcmp(type,"s")==0) {
+    learn_parm->type=OPTIMIZATION;
+    learn_parm->sharedslack=1;
+  }
   else {
     printf("\nUnknown type '%s': Valid types are 'c' (classification), 'r' regession, and 'p' preference ranking.\n",type);
     wait_any_key();
@@ -307,7 +297,7 @@ void wait_any_key()
 
 void print_help()
 {
-  printf("\nSVM-light %s: Support Vector Machine, learning module     %s\n",VERSION,VERSION_DATE);
+  printf("\nSVM-light %s: Support Vector Machine, learning module     %s\n",VERSION_SVMLIGHT,VERSION_DATE_SVMLIGHT);
   copyright_notice();
   printf("   usage: svm_learn [options] example_file model_file\n\n");
   printf("Arguments:\n");
@@ -363,12 +353,18 @@ void print_help()
   printf("                        The larger the faster...\n");
   printf("         -e float    -> eps: Allow that error for termination criterion\n");
   printf("                        [y [w*x+b] - 1] >= eps (default 0.001)\n");
+  printf("         -y [0,1]    -> restart the optimization from alpha values in file\n");
+  printf("                        specified by -a option. (default 0)\n");
   printf("         -h [5..]    -> number of iterations a variable needs to be\n"); 
   printf("                        optimal before considered for shrinking (default 100)\n");
   printf("         -f [0,1]    -> do final optimality check for variables removed\n");
   printf("                        by shrinking. Although this test is usually \n");
   printf("                        positive, there is no guarantee that the optimum\n");
   printf("                        was found if the test is omitted. (default 1)\n");
+  printf("         -y string   -> if option is given, reads alphas from file with given\n");
+  printf("                        and uses them as starting point. (default 'disabled')\n");
+  printf("         -# int      -> terminate optimization, if no progress after this\n");
+  printf("                        number of iterations. (default 100000)\n");
   printf("Output options:\n");
   printf("         -l string   -> file to write predicted labels of unlabeled\n");
   printf("                        examples into after transductive learning\n");
diff -rupN orig/svm_loqo.c svml/svm_loqo.c
--- orig/svm_loqo.c	2011-08-26 16:51:19.000000000 -0400
+++ svml/svm_loqo.c	2011-08-26 17:22:03.000000000 -0400
@@ -65,11 +65,17 @@ LEARN_PARM *learn_parm;
       for(j=0;j<qp->opt_n;j++) {
 	printf("%f ",qp->opt_g[i*qp->opt_n+j]);
       }
-      printf(": a=%.30f",qp->opt_xinit[i]);
+      printf(": a%ld=%.10f < %f",i,qp->opt_xinit[i],qp->opt_up[i]);
       printf(": y=%f\n",qp->opt_ce[i]);
     }
-    printf("\n");
-  }
+    for(j=0;j<qp->opt_m;j++) {
+      printf("EQ-%ld: %f*a0",j,qp->opt_ce[j]);
+      for(i=1;i<qp->opt_n;i++) {
+	printf(" + %f*a%ld",qp->opt_ce[i],i);
+      }
+      printf(" = %f\n\n",-qp->opt_ce0[0]);
+    }
+}
 
   obj_before=0; /* calculate objective before optimization */
   for(i=0;i<qp->opt_n;i++) {
@@ -115,7 +121,7 @@ LEARN_PARM *learn_parm;
       init_iter+=10;
       (opt_precision)*=10.0;   /* reduce precision */
       if(verbosity>=2) {
-	printf("NOTICE: Reducing precision of PR_LOQO.\n");
+	printf("NOTICE: Reducing precision of PR_LOQO due to (%ld).\n",result);
       }      
     }
   }
@@ -184,8 +190,9 @@ LEARN_PARM *learn_parm;
     }
   }
 
-  if(precision_violations > 50) { 
+  if(precision_violations > 500) { 
     (*epsilon_crit)*=10.0;
+    precision_violations=0;
     if(verbosity>=1) {
       printf("\nWARNING: Relaxing epsilon on KT-Conditions.\n");
     }
